{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stor/work/Wilke/afeller/PeptideMTR/venv312/lib/python3.12/site-packages/fastprogress/fastprogress.py:107: UserWarning: Couldn't import ipywidgets properly, progress bar will use console behavior\n",
      "  warn(\"Couldn't import ipywidgets properly, progress bar will use console behavior\")\n"
     ]
    }
   ],
   "source": [
    "from my_tokenizer import SMILES_SPE_Tokenizer\n",
    "\n",
    "tokenizer = SMILES_SPE_Tokenizer(vocab_file='build_tokenizer/vocab.txt', spe_file='build_tokenizer/merges.txt')\n",
    "\n",
    "sequence = 'N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O'\n",
    "print(len(sequence))\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.json created successfully with 405 tokens!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read vocab.txt and create a dictionary\n",
    "vocab_file = \"build_tokenizer/vocab.txt\"\n",
    "vocab_json_file = \"build_tokenizer/vocab.json\"\n",
    "\n",
    "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tokens = [line.strip() for line in f if line.strip()]  # Remove empty lines\n",
    "\n",
    "# Create a dictionary where tokens are keys and indices are values\n",
    "vocab_dict = {token: idx for idx, token in enumerate(tokens)}\n",
    "keys = list(vocab_dict.keys())\n",
    "# Save as vocab.json\n",
    "with open(vocab_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"vocab.json created successfully with {len(vocab_dict)} tokens!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenized output: [('Br', (0, 2)), ('Cl', (2, 4)), ('C', (4, 5)), ('C', (5, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('C', (17, 18)), ('C', (18, 19)), ('C', (19, 20))]\n",
      "Encoded tokens: ['[CLS]', 'B', '[UNK]', 'C', '[UNK]', 'C', 'C', '[UNK]', 'C', '[UNK]', '[UNK]', 'C', '[UNK]', 'C', 'C', 'C', '[SEP]']\n",
      "Is 'Br' in vocab? True\n",
      "Is 'Cl' in vocab? True\n",
      "Is 'C' in vocab? True\n",
      "Is '[C@@H]' in vocab? True\n",
      "Is '[C@H]' in vocab? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115887/2689363038.py:11: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
      "  tokenizer = Tokenizer(BPE(vocab=vocab_path,\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers import processors\n",
    "\n",
    "# Paths to your files\n",
    "vocab_path = \"build_tokenizer/vocab.json\"\n",
    "merges_path = \"build_tokenizer/merges_cleaned.txt\"\n",
    "\n",
    "# Step 1: Load BPE model with vocab and merges\n",
    "tokenizer = Tokenizer(BPE(vocab=vocab_path,\n",
    "                          merges=merges_path, \n",
    "                          unk_token=\"[UNK]\",\n",
    "                          continuing_subword_prefix=\"\",\n",
    "                          end_of_word_suffix=\"\",\n",
    "                          fuse_unk=True,\n",
    "                          byte_fallback=False,\n",
    "                          ignore_merges=False))\n",
    "\n",
    "# Step 2: Define and set pre-tokenizer using regex pattern\n",
    "regex_pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\?|\\/\\/?|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "pre_tokenizer = pre_tokenizers.Split(Regex(regex_pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Step 4: Add special tokens and configure padding/truncation (optional)\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=768)\n",
    "\n",
    "# Step 5: Add post-processor for special tokens\n",
    "seq_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokenizer.post_processor = seq_processor\n",
    "\n",
    "# Step 6: Save tokenizer in Hugging Face format (optional)\n",
    "tokenizer.save(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# Test tokenizer functionality\n",
    "test_sequence = \"BrClCC[C@@H][C@H]CCC\"\n",
    "pre_tokenized_output = tokenizer.pre_tokenizer.pre_tokenize_str(test_sequence)\n",
    "print(\"Pre-tokenized output:\", pre_tokenized_output)\n",
    "\n",
    "encoded_output = tokenizer.encode(test_sequence)\n",
    "print(\"Encoded tokens:\", encoded_output.tokens)\n",
    "\n",
    "# Debugging: Check if specific tokens exist in the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"Is 'Br' in vocab?\", \"Br\" in vocab)\n",
    "print(\"Is 'Cl' in vocab?\", \"Cl\" in vocab)\n",
    "print(\"Is 'C' in vocab?\", \"C\" in vocab)\n",
    "print(\"Is '[C@@H]' in vocab?\", \"[C@@H]\" in vocab)\n",
    "print(\"Is '[C@H]' in vocab?\", \"[C@H]\" in vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sample: [('[PAD]', 0), ('[UNK]', 1), ('[CLS]', 2), ('[SEP]', 3), ('[MASK]', 4)]\n",
      "\n",
      "Testing tokenization:\n",
      "Pre-tokenized: [('Br', (0, 2)), ('Cl', (2, 4)), ('C', (4, 5)), ('C', (5, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('C', (17, 18)), ('C', (18, 19)), ('C', (19, 20))]\n",
      "Encoded tokens: ['[CLS]', 'Br', 'Cl', 'C', 'C', '[C@@H]', '[C@H]', 'C', 'C', 'C', '[SEP]']\n",
      "Token IDs: [2, 97, 42, 83, 83, 67, 141, 83, 83, 83, 3]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece  # Let's try WordPiece instead of BPE for now\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers import processors\n",
    "\n",
    "# First, let's load and check the vocabulary\n",
    "import json\n",
    "with open(\"build_tokenizer/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Create tokenizer with WordPiece model\n",
    "tokenizer = Tokenizer(WordPiece(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set up pre-tokenizer\n",
    "regex_pattern = r\"(\\[[^\\]]+]|Br|Cl|N|O|S|P|F|I|B|C|n|o|s|p|c|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "pre_tokenizer = pre_tokenizers.Split(Regex(regex_pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Add special tokens and post-processor\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=768)\n",
    "\n",
    "seq_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "tokenizer.post_processor = seq_processor\n",
    "\n",
    "# Test\n",
    "test_sequence = \"BrClCC[C@@H][C@H]CCC\"\n",
    "print(\"\\nTesting tokenization:\")\n",
    "print(\"Pre-tokenized:\", tokenizer.pre_tokenizer.pre_tokenize_str(test_sequence))\n",
    "encoded = tokenizer.encode(test_sequence)\n",
    "print(\"Encoded tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sample: [('[PAD]', 0), ('[UNK]', 1), ('[CLS]', 2), ('[SEP]', 3), ('[MASK]', 4)]\n",
      "Merges sample: [('=', 'O'), ('C', 'C'), ('N', 'C'), ('C', 'O'), ('c', 'c')]\n",
      "\n",
      "Testing tokenization:\n",
      "Pre-tokenized: [('Br', (0, 2)), ('Cl', (2, 4)), ('C', (4, 5)), ('C', (5, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('C', (17, 18)), ('C', (18, 19)), ('C', (19, 20))]\n",
      "Encoded tokens: ['[CLS]', 'B', '[UNK]', 'C', '[UNK]', 'C', 'C', '[UNK]', 'C', '[UNK]', '[UNK]', 'C', '[UNK]', 'C', 'C', 'C', '[SEP]']\n",
      "Token IDs: [2, 122, 1, 83, 1, 83, 83, 1, 83, 1, 1, 83, 1, 83, 83, 83, 3]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers import processors\n",
    "\n",
    "# First, let's check our input files\n",
    "import json\n",
    "with open(\"build_tokenizer/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "    print(\"Vocabulary sample:\", list(vocab.items())[:5])\n",
    "\n",
    "with open(\"build_tokenizer/merges_cleaned.txt\", \"r\") as f:\n",
    "    # make each line a tuple (first element is the first token, second element is the second token)\n",
    "    merges = [tuple(line.strip().split()) for line in f]\n",
    "    print(\"Merges sample:\", merges[:5])\n",
    "\n",
    "# Create BPE model with explicit vocab and merges\n",
    "tokenizer = Tokenizer(BPE(\n",
    "    vocab=vocab,\n",
    "    merges=merges,\n",
    "    cache_capacity=10000,\n",
    "    dropout=None,\n",
    "    unk_token=\"[UNK]\",\n",
    "    continuing_subword_prefix=\"\",\n",
    "    end_of_word_suffix=\"\",\n",
    "    fuse_unk=True\n",
    "))\n",
    "\n",
    "# Set up pre-tokenizer\n",
    "regex_pattern = r\"(\\[[^\\]]+]|Br|Cl|N|O|S|P|F|I|B|C|n|o|s|p|c|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "pre_tokenizer = pre_tokenizers.Split(Regex(regex_pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Add special tokens and post-processor\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=768)\n",
    "\n",
    "seq_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "tokenizer.post_processor = seq_processor\n",
    "\n",
    "# Test\n",
    "test_sequence = \"BrClCC[C@@H][C@H]CCC\"\n",
    "print(\"\\nTesting tokenization:\")\n",
    "print(\"Pre-tokenized:\", tokenizer.pre_tokenizer.pre_tokenize_str(test_sequence))\n",
    "encoded = tokenizer.encode(test_sequence)\n",
    "print(\"Encoded tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: CCC\n",
      "Pre-tokenized: [('CCC', (0, 3))]\n",
      "Final tokens: ['[CLS]', 'CCC', '[SEP]']\n",
      "\n",
      "Testing: CCCC\n",
      "Pre-tokenized: [('CCCC', (0, 4))]\n",
      "Final tokens: ['[CLS]', 'CCCC', '[SEP]']\n",
      "\n",
      "Testing: BrCCCB\n",
      "Pre-tokenized: [('Br', (0, 2)), ('CCC', (2, 5)), ('B', (5, 6))]\n",
      "Final tokens: ['[CLS]', 'Br', 'CCC', 'B', '[SEP]']\n",
      "\n",
      "Testing: BrClCC[C@@H][C@H]CCC\n",
      "Pre-tokenized: [('Br', (0, 2)), ('Cl', (2, 4)), ('CC', (4, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('CCC', (17, 20))]\n",
      "Final tokens: ['[CLS]', 'Br', 'Cl', 'CC', '[C@@H]', '[C@H]', 'CCC', '[SEP]']\n",
      "\n",
      "Testing: C(=O)N\n",
      "Pre-tokenized: [('C(=O)N', (0, 6))]\n",
      "Final tokens: ['[CLS]', 'C(=O)N', '[SEP]']\n",
      "\n",
      "Testing: CC(=O)CC\n",
      "Pre-tokenized: [('CC(=O)', (0, 6)), ('CC', (6, 8))]\n",
      "Final tokens: ['[CLS]', 'CC(=O)', 'CC', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex, processors\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "# Create WordPiece tokenizer\n",
    "tokenizer = Tokenizer(WordPiece(\n",
    "    vocab=vocab,\n",
    "    unk_token=\"[UNK]\",\n",
    "    max_input_chars_per_word=100\n",
    "))\n",
    "\n",
    "# Chemical regex pattern with correct escaping\n",
    "chemical_pattern = r\"(\\[[^\\]]+]|C\\(=N\\)N|CCC\\(C\\)|\\(CCCN\\)|NC\\(=O\\)|C\\(C\\)=O|=C\\(N\\)N|N=C\\(N\\)|NC\\(=N\\)|C\\(=O\\)C|CS\\(=O\\)|OC\\(=O\\)|C\\(=O\\)c|c\\(=O\\)n|C\\(=O\\)O|C\\(N\\)=O|cc\\(Br\\)|CC\\(=O\\)|C\\(=O\\)N|ccc\\(C\\)|ccc\\(F\\)|c\\(=O\\)|C\\(=N\\)|c\\(O\\)c|NC\\(C\\)|n\\(C\\)c|CC\\(O\\)|cc\\(N\\)|CC\\(C\\)|cc\\(C\\)|C\\(=O\\)|cc\\(O\\)|c\\(N\\)c|c\\(Cl\\)|C\\(N\\)N|N\\(C\\)C|NC\\(N\\)|=C\\(N\\)|C\\(O\\)C|c\\(OC\\)|\\(C#N\\)|C\\(C\\)C|CC\\(N\\)|C\\(C\\)N|c\\(CO\\)|c\\(Br\\)|\\(CCO\\)|C\\(CC\\)|S\\(=O\\)|c\\(C\\)c|\\(=N\\)|c\\(O\\)|\\(Br\\)|\\(CS\\)|c\\(C\\)|\\(CC\\)|c\\(I\\)|C\\(C\\)|N\\(C\\)|C\\(O\\)|C\\(I\\)|C\\(F\\)|\\(Cl\\)|n\\(C\\)|\\(OC\\)|\\(=O\\)|c\\(F\\)|CCCN\\)|\\(=S\\)|c\\(N\\)|\\(CO\\)|C\\(N\\)|\\(C\\)|ccccc|\\(S\\)|\\(F\\)|\\(O\\)|C#N\\)|CCO\\)|\\(N\\)|C\\(=N|\\(I\\)|CSSC|=N\\)|CC=O|CCCO|Cl\\)|CCNO|=O\\)|CCSC|\\(=N|CO\\)|CCNC|CCCC|=S\\)|CN=C|CCCS|cccc|CCCN|Br\\)|cccn|CS\\)|C=CC|OC\\)|CC=C|cnn|=NC|COC|OCC|\\(O|CCS|CNc|#Cc|=CC|ccn|C=C|CSc|ccc|NCc|CCO|N=C|cnc|I\\)|CCc|OCc|CCl|ccs|COc|CCn|CSC|SCC|NCC|CCN|CNC|C#C|C=O|CNO|CCC|SSC|C#N|O=C|NOC|S\\)|csc|ncc|C\\)|N\\)|\\(C|ncn|F\\)|O\\)|N#C|nnc|CSS|cco|Cl|NC|nc|co|CS|CO|no|cc|CN|cn|SS|OC|\\)|SN|nn|CC|#C|NO|=S|NS|cs|=C|Oc|=O|oc|Nc|Cc|=N|NN|C=|C#|\\(|SC|sc|Br|N#|#N|p|O|I|N|C|s|=|c|B|S|F|n|P|#|o)\"\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Split(Regex(chemical_pattern), behavior=\"isolated\")\n",
    "\n",
    "# Add post-processor for special tokens\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# Test sequences\n",
    "test_sequences = [\n",
    "    \"CCC\",\n",
    "    \"CCCC\",\n",
    "    \"BrCCCB\",\n",
    "    \"BrClCC[C@@H][C@H]CCC\",\n",
    "    \"C(=O)N\",  # Test parentheses\n",
    "    \"CC(=O)CC\"  # Test more complex structure\n",
    "]\n",
    "\n",
    "for seq in test_sequences:\n",
    "    print(f\"\\nTesting: {seq}\")\n",
    "    pre_tokens = tokenizer.pre_tokenizer.pre_tokenize_str(seq)\n",
    "    print(\"Pre-tokenized:\", pre_tokens)\n",
    "    encoded = tokenizer.encode(seq)\n",
    "    print(\"Final tokens:\", encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "286\n",
      "Compression ratio: 3.3986013986013988\n",
      "N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# test the tokenizer\n",
    "sequence = 'N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O'\n",
    "print(len(sequence))\n",
    "\n",
    "# tokenize the sequence\n",
    "tokens = tokenizer.encode(sequence)\n",
    "print(len(tokens))\n",
    "\n",
    "print(\"Compression ratio:\", len(sequence) / len(tokens))\n",
    "\n",
    "# decode the tokens\n",
    "decoded = tokenizer.decode(tokens.ids)\n",
    "# replace all spaces with ''\n",
    "decoded = decoded.replace(\" \", \"\")\n",
    "# remove [CLS] and [SEP]\n",
    "decoded = decoded.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")\n",
    "\n",
    "print(decoded)\n",
    "\n",
    "\n",
    "# check if decoded is the same as sequence\n",
    "print(decoded == sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amino acids: 20\n",
      "SMILES: 399\n",
      "Tokens: 118\n",
      "['[CLS]', 'CC', '[C@H]', '(C)', '[C@H]', '(', 'NC(=O)', '[C@H]', '(C', 'c', '1', 'c', '[nH]', 'cn', '1', ')', 'NC(=O)', 'CNC', '(=O)', '[C@H]', '(C', 'CC(N)', '=O)', 'NC(=O)', '[C@H]', '(C', 'CC(=O)', 'O)', 'NC(=O)', '[C@H]', '(CS)', 'NC(=O)', '[C@H]', '(C', 'C(=O)O', ')', 'NC(=O)', '[C@H]', '(C', 'C(N)=O', ')', 'NC(=O)', '[C@H]', '(C', 'CCNC', '(=N)', 'N)', 'NC(=O)', '[C@H]', '(C)', 'N)', 'C(=O)N', '[C@@H]', '(C', 'C(C)C', ')', 'C(=O)N', '[C@@H]', '(C', 'CCCN)', 'C(=O)N', '[C@@H]', '(C', 'CSC', ')', 'C(=O)N', '[C@@H]', '(C', 'c', '1', 'ccccc', '1', ')', 'C(=O)N', '1', 'CCC', '[C@H]', '1', 'C(=O)N', '[C@@H]', '(CO)', 'C(=O)N', '[C@H]', '(C', '(=O)', 'N', '[C@@H]', '(C', 'c', '1', 'c', '[nH]', 'c', '2', 'ccccc', '[UNK]', ')', 'C(=O)N', '[C@@H]', '(C', 'c', '1', 'ccc', '(O)', 'cc', '1', ')', 'C(=O)N', '[C@H]', '(C', '(=O)', 'O)', 'C(C)C', ')', '[C@@H]', '(C)', 'O', '[SEP]']\n",
      "Compression ratio: 3.3813559322033897\n",
      "Tokens per amino acid: 5.9\n",
      "CC[C@H](C)[C@H](NC(=O)[C@H](Cc1c[nH]cn1)NC(=O)CNC(=O)[C@H](CCC(N)=O)NC(=O)[C@H](CCC(=O)O)NC(=O)[C@H](CS)NC(=O)[C@H](CC(=O)O)NC(=O)[C@H](CC(N)=O)NC(=O)[C@H](CCCNC(=N)N)NC(=O)[C@H](C)N)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](Cc1ccccc1)C(=O)N1CCC[C@H]1C(=O)N[C@@H](CO)C(=O)N[C@H](C(=O)N[C@@H](Cc1c[nH]c2ccccc[UNK])C(=O)N[C@@H](Cc1ccc(O)cc1)C(=O)N[C@H](C(=O)O)C(C)C)[C@@H](C)O\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from rdkit import Chem\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# test the tokenizer\n",
    "sequence = 'ARNDCEQGHILKMFPSTWYV'\n",
    "print(f\"Amino acids: {len(sequence)}\")\n",
    "\n",
    "smiles = Chem.MolToSmiles(Chem.MolFromFASTA(sequence))\n",
    "print(f\"SMILES: {len(smiles)}\")\n",
    "\n",
    "# tokenize the sequence\n",
    "tokens = tokenizer.encode(smiles)\n",
    "print(f\"Tokens: {len(tokens)}\")\n",
    "print(tokens.tokens)\n",
    "print(\"Compression ratio:\", len(smiles) / len(tokens))\n",
    "\n",
    "print(\"Tokens per amino acid:\", len(tokens) / len(sequence))\n",
    "\n",
    "# decode the tokens\n",
    "decoded = tokenizer.decode(tokens.ids)\n",
    "# replace all spaces with ''\n",
    "decoded = decoded.replace(\" \", \"\")\n",
    "# remove [CLS] and [SEP]\n",
    "decoded = decoded.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")\n",
    "\n",
    "print(decoded)\n",
    "\n",
    "\n",
    "# check if decoded is the same as sequence\n",
    "print(decoded == sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[CH]': 110,\n",
       " '[C]': 132,\n",
       " 'c(O)c': 363,\n",
       " '[N@@+]': 133,\n",
       " '[Ra]': 11,\n",
       " '[I+3]': 188,\n",
       " 'cccc': 230,\n",
       " '[Ag+]': 166,\n",
       " 'n': 68,\n",
       " '[FH]': 183,\n",
       " '[14CH2]': 35,\n",
       " '[BH-]': 59,\n",
       " '[Ga]': 142,\n",
       " '[PH+]': 74,\n",
       " 'NC(=O)': 255,\n",
       " '(=S)': 323,\n",
       " 'F)': 265,\n",
       " 'CCCN)': 260,\n",
       " 'CO)': 249,\n",
       " '/': 22,\n",
       " '[11C]': 55,\n",
       " '[BrH]': 178,\n",
       " '=C(N)': 380,\n",
       " '[S]': 206,\n",
       " 'N#': 337,\n",
       " 'CCSC': 273,\n",
       " '(C': 239,\n",
       " '[Al-3]': 169,\n",
       " '(CS)': 357,\n",
       " '7': 160,\n",
       " '9': 46,\n",
       " '[Cl+2]': 43,\n",
       " '[Po]': 10,\n",
       " 'co': 335,\n",
       " '[N@+]': 79,\n",
       " 'C(N)N': 314,\n",
       " '[11CH3]': 28,\n",
       " '[Ag-4]': 167,\n",
       " 'CN': 228,\n",
       " '%15': 104,\n",
       " '[C+]': 77,\n",
       " '[N@]': 76,\n",
       " '[te]': 96,\n",
       " 'cco': 318,\n",
       " '[Sn+]': 30,\n",
       " '[Ca]': 88,\n",
       " '[Al]': 45,\n",
       " 'CC': 219,\n",
       " '3': 26,\n",
       " '[OH]': 163,\n",
       " '%13': 111,\n",
       " '[CH2-]': 154,\n",
       " '[n-]': 134,\n",
       " '[P]': 202,\n",
       " '#C': 317,\n",
       " '=S': 333,\n",
       " 'ncn': 303,\n",
       " '[S@@+]': 143,\n",
       " 'CC(=O)': 243,\n",
       " 'C=C': 293,\n",
       " '[Si]': 37,\n",
       " 'ccn': 275,\n",
       " '[CaH2]': 181,\n",
       " 'cnc': 359,\n",
       " 'C(N)': 246,\n",
       " 'o': 131,\n",
       " 'sc': 340,\n",
       " '[3H]': 12,\n",
       " '[CH2]': 123,\n",
       " '[Xe]': 212,\n",
       " 'ccc(C)': 301,\n",
       " 'NC(=N)': 262,\n",
       " 'CC(N)': 247,\n",
       " 'C)': 269,\n",
       " 'N(C)': 312,\n",
       " '1': 107,\n",
       " '=O)': 234,\n",
       " 'COC': 365,\n",
       " '[As+]': 152,\n",
       " '(CCO)': 398,\n",
       " '[AsH]': 171,\n",
       " '=O': 218,\n",
       " '(N)': 233,\n",
       " '[Br+2]': 177,\n",
       " '[K]': 190,\n",
       " 'CCn': 387,\n",
       " '[S+]': 130,\n",
       " 'c(F)': 343,\n",
       " 'c(N)': 354,\n",
       " 'cc(C)': 369,\n",
       " 'no': 336,\n",
       " 'cs': 371,\n",
       " 'oc': 334,\n",
       " 'CO': 221,\n",
       " '[Hg]': 100,\n",
       " 'p': 164,\n",
       " '(Cl)': 361,\n",
       " '8': 14,\n",
       " '=CC': 308,\n",
       " 'SCC': 285,\n",
       " '[CLS]': 2,\n",
       " 'cn': 274,\n",
       " '[UNK]': 1,\n",
       " '[I+]': 109,\n",
       " '[Th]': 82,\n",
       " 'C(=O)c': 374,\n",
       " '[Se]': 155,\n",
       " '=C': 278,\n",
       " '[NH2+]': 50,\n",
       " '(O)': 270,\n",
       " 'COc': 373,\n",
       " '[n]': 216,\n",
       " '5': 85,\n",
       " '[C@H]': 141,\n",
       " '[Ru]': 60,\n",
       " '(C#N)': 331,\n",
       " ')': 117,\n",
       " 'CCO)': 397,\n",
       " '[NH+]': 108,\n",
       " 'C(=O)': 236,\n",
       " 'CC=C': 385,\n",
       " 'C=O': 342,\n",
       " '%19': 41,\n",
       " 'NN': 390,\n",
       " '[NH-]': 120,\n",
       " 'OC)': 350,\n",
       " '[OH+]': 116,\n",
       " '[V]': 62,\n",
       " '[cH-]': 81,\n",
       " '[Pb]': 112,\n",
       " '[Te]': 127,\n",
       " '[Tl]': 90,\n",
       " 'Br)': 263,\n",
       " 'CS)': 356,\n",
       " '(O': 280,\n",
       " 'CSc': 391,\n",
       " 'NC(N)': 349,\n",
       " '[H]': 185,\n",
       " '[He]': 186,\n",
       " '[SH2]': 205,\n",
       " '(OC)': 351,\n",
       " '[123I]': 34,\n",
       " '[125I]': 80,\n",
       " ':': 5,\n",
       " '[N@@]': 99,\n",
       " '[Os]': 162,\n",
       " '[S-]': 13,\n",
       " '[Si@]': 209,\n",
       " '6': 135,\n",
       " '[11CH]': 51,\n",
       " '[N]': 197,\n",
       " 's': 73,\n",
       " '[Na]': 198,\n",
       " '[SiH3]': 53,\n",
       " '[BH2-]': 93,\n",
       " '[MgH2]': 194,\n",
       " '(C)': 240,\n",
       " 'CCO': 279,\n",
       " '[c+]': 156,\n",
       " 'nc': 339,\n",
       " '[c-]': 86,\n",
       " 'CSS': 306,\n",
       " 'csc': 300,\n",
       " 'C(=O)N': 237,\n",
       " 'Br': 97,\n",
       " 'C(=O)C': 245,\n",
       " '=S)': 322,\n",
       " '[13C]': 29,\n",
       " 'c(OC)': 352,\n",
       " 'n(C)c': 401,\n",
       " '4': 21,\n",
       " '=NC': 404,\n",
       " '[SiH]': 145,\n",
       " 'c(Br)': 353,\n",
       " '=N)': 256,\n",
       " 'nn': 370,\n",
       " '#N': 295,\n",
       " 'OCc': 384,\n",
       " 'C(=N)N': 341,\n",
       " '(': 149,\n",
       " '[NH3]': 196,\n",
       " '(=N': 281,\n",
       " 'CS(=O)': 393,\n",
       " '[As]': 8,\n",
       " '[IH]': 17,\n",
       " '%18': 58,\n",
       " '#Cc': 403,\n",
       " 'SSC': 327,\n",
       " '[O]': 20,\n",
       " '[P+]': 124,\n",
       " '[S@@]': 136,\n",
       " '[SnH]': 52,\n",
       " '[SEP]': 3,\n",
       " 'ccc(F)': 286,\n",
       " 'ccs': 297,\n",
       " 'c(Cl)': 362,\n",
       " '[Tc]': 118,\n",
       " 'NO': 298,\n",
       " 'N)': 232,\n",
       " '[Cs]': 182,\n",
       " '[Mg]': 195,\n",
       " '[AsH3]': 170,\n",
       " '[Se+]': 128,\n",
       " '[Se-]': 70,\n",
       " 'cc': 222,\n",
       " '=N': 253,\n",
       " '[SeH2]': 208,\n",
       " '[I+2]': 187,\n",
       " '[F]': 184,\n",
       " '[nH+]': 137,\n",
       " '[C-]': 106,\n",
       " 'c(C)': 375,\n",
       " '[Se-2]': 207,\n",
       " 'I)': 309,\n",
       " '[11c]': 146,\n",
       " 'O': 150,\n",
       " '[O-]': 94,\n",
       " 'C(CC)': 394,\n",
       " '[CH-]': 16,\n",
       " 'CCNC': 254,\n",
       " 'Cl': 42,\n",
       " '[P@]': 115,\n",
       " 'N#C': 338,\n",
       " '[BH3-]': 33,\n",
       " 'C=CC': 381,\n",
       " '[B@-]': 173,\n",
       " '[Cr]': 98,\n",
       " '[O+]': 64,\n",
       " 'c(C)c': 372,\n",
       " 'c(CO)': 402,\n",
       " 'ncc': 368,\n",
       " '[P@+]': 165,\n",
       " 'C(=N': 282,\n",
       " '[18F]': 91,\n",
       " 'C(F)': 344,\n",
       " 'OCC': 366,\n",
       " '(F)': 266,\n",
       " 'c': 65,\n",
       " '%12': 129,\n",
       " '[Li+]': 192,\n",
       " '[Ge]': 32,\n",
       " 'CS': 252,\n",
       " 'P': 18,\n",
       " '[At]': 172,\n",
       " '[LiH]': 193,\n",
       " '%16': 89,\n",
       " 'C': 83,\n",
       " 'Nc': 367,\n",
       " '[SH]': 158,\n",
       " '[SrH2]': 210,\n",
       " '[s+]': 78,\n",
       " '[C@@]': 27,\n",
       " 'CCCS': 272,\n",
       " 'NS': 319,\n",
       " '[Zn+2]': 213,\n",
       " '[se]': 125,\n",
       " 'CCCN': 227,\n",
       " 'S)': 267,\n",
       " '[TlH2]': 49,\n",
       " '[N+]': 71,\n",
       " '[pH]': 9,\n",
       " '=C(N)N': 382,\n",
       " 'CCN': 226,\n",
       " 'CCc': 289,\n",
       " 'NC': 220,\n",
       " 'C(C)N': 396,\n",
       " '[B-]': 47,\n",
       " 'OC': 284,\n",
       " '[Bi]': 95,\n",
       " 'CN=C': 399,\n",
       " '[OH2]': 199,\n",
       " 'C(O)C': 379,\n",
       " '[NH3+]': 144,\n",
       " '\\\\': 102,\n",
       " 'c(=O)n': 321,\n",
       " '[Ba]': 176,\n",
       " '(I)': 310,\n",
       " '[14C]': 113,\n",
       " '(CCCN)': 261,\n",
       " 'C=': 294,\n",
       " 'NCc': 377,\n",
       " '[N-]': 23,\n",
       " 'CCl': 386,\n",
       " 'CCS': 271,\n",
       " 'SS': 332,\n",
       " 'C#N)': 330,\n",
       " 'F': 153,\n",
       " 'SN': 320,\n",
       " '[B@@-]': 174,\n",
       " 'ccc': 225,\n",
       " '[C@@H]': 67,\n",
       " 'N(C)C': 313,\n",
       " 'CC=O': 259,\n",
       " '(S)': 268,\n",
       " 'CSSC': 307,\n",
       " '[SiH2]': 57,\n",
       " 'CNO': 311,\n",
       " '[Cl+3]': 25,\n",
       " 'Cc': 292,\n",
       " '[2H]': 114,\n",
       " 'c(I)': 345,\n",
       " 'O)': 238,\n",
       " 'S(=O)': 287,\n",
       " 'C#C': 395,\n",
       " 'C(=O)O': 244,\n",
       " 'NOC': 315,\n",
       " 'C#N': 296,\n",
       " 'C(C)': 241,\n",
       " '[C@]': 75,\n",
       " '[SH+]': 204,\n",
       " '[IH2]': 151,\n",
       " '[TeH]': 211,\n",
       " 'C(I)': 346,\n",
       " '[n+]': 103,\n",
       " '[I]': 189,\n",
       " '[te+]': 217,\n",
       " '[PAD]': 0,\n",
       " '#': 87,\n",
       " '(=O)': 235,\n",
       " '%23': 157,\n",
       " 'I': 159,\n",
       " '%10': 61,\n",
       " '[In]': 126,\n",
       " '[P@@+]': 200,\n",
       " '[Sn]': 54,\n",
       " '%11': 6,\n",
       " 'ccccc': 231,\n",
       " 'n(C)': 400,\n",
       " 'nnc': 378,\n",
       " '[Zn-2]': 214,\n",
       " 'N': 72,\n",
       " 'cnn': 347,\n",
       " '=': 40,\n",
       " '[123Te]': 105,\n",
       " '[PH]': 139,\n",
       " '%17': 148,\n",
       " 'O=C': 288,\n",
       " '[B]': 175,\n",
       " '[Zn]': 215,\n",
       " 'C(=N)': 258,\n",
       " '(=N)': 257,\n",
       " 'CCC(C)': 328,\n",
       " 'CCCC': 224,\n",
       " '[I-]': 66,\n",
       " 'cc(O)': 355,\n",
       " 'CCC': 223,\n",
       " '[Br]': 179,\n",
       " 'CC(C)': 251,\n",
       " 'C(N)=O': 248,\n",
       " 'CCCO': 305,\n",
       " 'c(N)c': 324,\n",
       " '[Ag]': 168,\n",
       " '[Rb]': 203,\n",
       " '[W]': 39,\n",
       " 'C(C)=O': 316,\n",
       " '%22': 161,\n",
       " 'C(O)': 299,\n",
       " '[Al-]': 36,\n",
       " '%14': 44,\n",
       " '(Br)': 264,\n",
       " '[Hg+]': 140,\n",
       " '[o+]': 63,\n",
       " '-': 7,\n",
       " 'cc(Br)': 302,\n",
       " 'Cl)': 360,\n",
       " '[PH2]': 201,\n",
       " 'CC(O)': 389,\n",
       " '[CH3]': 180,\n",
       " '[Kr]': 191,\n",
       " '[MASK]': 4,\n",
       " 'NC(C)': 392,\n",
       " '[S@]': 38,\n",
       " '[Si+]': 138,\n",
       " '[129Xe]': 24,\n",
       " '[P@@]': 31,\n",
       " '[S@+]': 101,\n",
       " 'N=C(N)': 383,\n",
       " '%21': 15,\n",
       " 'N=C': 325,\n",
       " 'NCC': 364,\n",
       " 'SC': 326,\n",
       " '[Cl+]': 48,\n",
       " 'c(O)': 283,\n",
       " 'cc(N)': 348,\n",
       " 'C#': 291,\n",
       " '[se+]': 119,\n",
       " '%20': 147,\n",
       " 'CNc': 388,\n",
       " '2': 69,\n",
       " 'CNC': 229,\n",
       " 'OC(=O)': 290,\n",
       " 'S': 56,\n",
       " '(CC)': 376,\n",
       " 'C(C)C': 242,\n",
       " 'Oc': 358,\n",
       " '[Sb]': 84,\n",
       " '(CO)': 250,\n",
       " 'cccn': 276,\n",
       " 'c(=O)': 329,\n",
       " 'B': 122,\n",
       " 'CSC': 277,\n",
       " '[nH]': 121,\n",
       " '[SeH]': 19,\n",
       " 'CCNO': 304,\n",
       " '[223Ra]': 92}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(tokenizer.get_vocab()))\n",
    "tokenizer.get_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc65c32736848bba191890a72d7be48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aaronfeller/PeptideMTR/commit/bbd6290716d787242ecbfa325211474cbb8b6161', commit_message='Add chemical WordPiece tokenizer', commit_description='', oid='bbd6290716d787242ecbfa325211474cbb8b6161', pr_url=None, repo_url=RepoUrl('https://huggingface.co/aaronfeller/PeptideMTR', endpoint='https://huggingface.co', repo_type='model', repo_id='aaronfeller/PeptideMTR'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex, processors\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import pre_tokenizers\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# First, convert your tokenizer to a PreTrainedTokenizerFast\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "# Add tokenizer metadata\n",
    "fast_tokenizer.name_or_path = \"chemical-wordpiece-tokenizer\"\n",
    "fast_tokenizer.model_max_length = 768  # or whatever max length you want\n",
    "\n",
    "# Push to hub (make sure you're logged in first)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Push to hub\n",
    "fast_tokenizer.push_to_hub(\n",
    "    \"aaronfeller/PeptideMTR\",  # e.g., \"pharmapsychotic/chemical-wordpiece\"\n",
    "    commit_message=\"Add chemical WordPiece tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 107, 67, 239, 219, 107, 117, 237, 141, 239, 244, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 250, 237, 67, 149, 141, 376, 269, 237, 67, 239, 65, 107, 231, 107, 117, 237, 67, 239, 242, 117, 237, 67, 239, 237, 26, 117, 237, 67, 239, 242, 117, 237, 67, 250, 237, 67, 239, 65, 107, 231, 107, 117, 237, 243, 72, 141, 239, 219, 141, 239, 235, 238, 232, 237, 67, 239, 237, 117, 237, 141, 239, 219, 141, 239, 235, 238, 232, 237, 67, 239, 83, 149, 278, 72, 69, 117, 83, 107, 278, 69, 381, 278, 107, 117, 237, 67, 239, 277, 117, 237, 67, 239, 277, 117, 237, 67, 149, 141, 270, 269, 237, 67, 239, 242, 117, 237, 67, 250, 237, 67, 240, 237, 67, 239, 243, 238, 237, 67, 239, 254, 257, 232, 237, 67, 239, 240, 269, 237, 67, 239, 237, 117, 237, 67, 239, 254, 257, 232, 237, 67, 239, 244, 117, 237, 67, 239, 65, 107, 231, 107, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 239, 254, 257, 232, 237, 67, 239, 277, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 239, 240, 269, 237, 67, 149, 141, 270, 244, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 239, 243, 238, 237, 67, 240, 237, 141, 357, 237, 67, 239, 65, 107, 231, 107, 360, 237, 67, 239, 243, 232, 237, 67, 239, 277, 117, 237, 67, 239, 243, 232, 237, 67, 239, 254, 257, 232, 237, 67, 239, 65, 107, 65, 121, 65, 69, 65, 107, 324, 222, 69, 117, 244, 3]\n",
      "N 1 [C@@H] (C CC 1 ) C(=O)N [C@H] (C C(=O)O ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (CO) C(=O)N [C@@H] ( [C@H] (CC) C) C(=O)N [C@@H] (C c 1 ccccc 1 ) C(=O)N [C@@H] (C C(C)C ) C(=O)N [C@@H] (C C(=O)N 3 ) C(=O)N [C@@H] (C C(C)C ) C(=O)N [C@@H] (CO) C(=O)N [C@@H] (C c 1 ccccc 1 ) C(=O)N CC(=O) N [C@H] (C CC [C@H] (C (=O) O) N) C(=O)N [C@@H] (C C(=O)N ) C(=O)N [C@H] (C CC [C@H] (C (=O) O) N) C(=O)N [C@@H] (C C ( =C N 2 ) C 1 =C 2 C=CC =C 1 ) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] ( [C@H] (O) C) C(=O)N [C@@H] (C C(C)C ) C(=O)N [C@@H] (CO) C(=O)N [C@@H] (C) C(=O)N [C@@H] (C CC(=O) O) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C (C) C) C(=O)N [C@@H] (C C(=O)N ) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C C(=O)O ) C(=O)N [C@@H] (C c 1 ccccc 1 ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (C (C) C) C(=O)N [C@@H] ( [C@H] (O) C(=O)O ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (C CC(=O) O) C(=O)N [C@@H] (C) C(=O)N [C@H] (CS) C(=O)N [C@@H] (C c 1 ccccc 1 Cl) C(=O)N [C@@H] (C CC(=O) N) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] (C CC(=O) N) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C c 1 c [nH] c 2 c 1 c(N)c cc 2 ) C(=O)O\n",
      "True\n",
      "SMILES length: 972\n",
      "Token length: 286\n",
      "Compression ratio: 3.3986013986013988\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# load the tokenizer from the hub\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"aaronfeller/PeptideMTR\")\n",
    "\n",
    "tokenizer.encode(\"CCC\")\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"CCC\"))\n",
    "\n",
    "sequence = 'N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O'\n",
    "\n",
    "tokens = tokenizer.encode(sequence)\n",
    "print(tokens)\n",
    "\n",
    "decoded = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "\n",
    "# remove spaces in decoded\n",
    "decoded = decoded.replace(\" \", \"\")\n",
    "\n",
    "print(decoded == sequence)\n",
    "\n",
    "# check compression ratio\n",
    "print(\"SMILES length:\", len(sequence))\n",
    "print(\"Token length:\", len(tokens))\n",
    "print(\"Compression ratio:\", len(sequence) / len(tokens))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
