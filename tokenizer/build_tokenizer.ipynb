{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stor/work/Wilke/afeller/PeptideMTR/venv312/lib/python3.12/site-packages/fastprogress/fastprogress.py:107: UserWarning: Couldn't import ipywidgets properly, progress bar will use console behavior\n",
      "  warn(\"Couldn't import ipywidgets properly, progress bar will use console behavior\")\n"
     ]
    }
   ],
   "source": [
    "from my_tokenizer import SMILES_SPE_Tokenizer\n",
    "\n",
    "tokenizer = SMILES_SPE_Tokenizer(vocab_file='build_tokenizer/vocab.txt', spe_file='build_tokenizer/merges.txt')\n",
    "\n",
    "sequence = 'N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O'\n",
    "print(len(sequence))\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.json created successfully with 405 tokens!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read vocab.txt and create a dictionary\n",
    "vocab_file = \"build_tokenizer/vocab.txt\"\n",
    "vocab_json_file = \"build_tokenizer/vocab.json\"\n",
    "\n",
    "with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tokens = [line.strip() for line in f if line.strip()]  # Remove empty lines\n",
    "\n",
    "# Create a dictionary where tokens are keys and indices are values\n",
    "vocab_dict = {token: idx for idx, token in enumerate(tokens)}\n",
    "keys = list(vocab_dict.keys())\n",
    "# Save as vocab.json\n",
    "with open(vocab_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"vocab.json created successfully with {len(vocab_dict)} tokens!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenized output: [('Br', (0, 2)), ('Cl', (2, 4)), ('C', (4, 5)), ('C', (5, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('C', (17, 18)), ('C', (18, 19)), ('C', (19, 20))]\n",
      "Encoded tokens: ['[CLS]', 'B', '[UNK]', 'C', '[UNK]', 'C', 'C', '[UNK]', 'C', '[UNK]', '[UNK]', 'C', '[UNK]', 'C', 'C', 'C', '[SEP]']\n",
      "Is 'Br' in vocab? True\n",
      "Is 'Cl' in vocab? True\n",
      "Is 'C' in vocab? True\n",
      "Is '[C@@H]' in vocab? True\n",
      "Is '[C@H]' in vocab? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115887/2689363038.py:11: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
      "  tokenizer = Tokenizer(BPE(vocab=vocab_path,\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers import processors\n",
    "\n",
    "# Paths to your files\n",
    "vocab_path = \"build_tokenizer/vocab.json\"\n",
    "merges_path = \"build_tokenizer/merges_cleaned.txt\"\n",
    "\n",
    "# Step 1: Load BPE model with vocab and merges\n",
    "tokenizer = Tokenizer(BPE(vocab=vocab_path,\n",
    "                          merges=merges_path, \n",
    "                          unk_token=\"[UNK]\",\n",
    "                          continuing_subword_prefix=\"\",\n",
    "                          end_of_word_suffix=\"\",\n",
    "                          fuse_unk=True,\n",
    "                          byte_fallback=False,\n",
    "                          ignore_merges=False))\n",
    "\n",
    "# Step 2: Define and set pre-tokenizer using regex pattern\n",
    "regex_pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\?|\\/\\/?|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "pre_tokenizer = pre_tokenizers.Split(Regex(regex_pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Step 4: Add special tokens and configure padding/truncation (optional)\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=768)\n",
    "\n",
    "# Step 5: Add post-processor for special tokens\n",
    "seq_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokenizer.post_processor = seq_processor\n",
    "\n",
    "# Step 6: Save tokenizer in Hugging Face format (optional)\n",
    "tokenizer.save(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# Test tokenizer functionality\n",
    "test_sequence = \"BrClCC[C@@H][C@H]CCC\"\n",
    "pre_tokenized_output = tokenizer.pre_tokenizer.pre_tokenize_str(test_sequence)\n",
    "print(\"Pre-tokenized output:\", pre_tokenized_output)\n",
    "\n",
    "encoded_output = tokenizer.encode(test_sequence)\n",
    "print(\"Encoded tokens:\", encoded_output.tokens)\n",
    "\n",
    "# Debugging: Check if specific tokens exist in the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"Is 'Br' in vocab?\", \"Br\" in vocab)\n",
    "print(\"Is 'Cl' in vocab?\", \"Cl\" in vocab)\n",
    "print(\"Is 'C' in vocab?\", \"C\" in vocab)\n",
    "print(\"Is '[C@@H]' in vocab?\", \"[C@@H]\" in vocab)\n",
    "print(\"Is '[C@H]' in vocab?\", \"[C@H]\" in vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sample: [('[PAD]', 0), ('[UNK]', 1), ('[CLS]', 2), ('[SEP]', 3), ('[MASK]', 4)]\n",
      "\n",
      "Testing tokenization:\n",
      "Pre-tokenized: [('Br', (0, 2)), ('Cl', (2, 4)), ('C', (4, 5)), ('C', (5, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('C', (17, 18)), ('C', (18, 19)), ('C', (19, 20))]\n",
      "Encoded tokens: ['[CLS]', 'Br', 'Cl', 'C', 'C', '[C@@H]', '[C@H]', 'C', 'C', 'C', '[SEP]']\n",
      "Token IDs: [2, 97, 42, 83, 83, 67, 141, 83, 83, 83, 3]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece  # Let's try WordPiece instead of BPE for now\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers import processors\n",
    "\n",
    "# First, let's load and check the vocabulary\n",
    "import json\n",
    "with open(\"build_tokenizer/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Create tokenizer with WordPiece model\n",
    "tokenizer = Tokenizer(WordPiece(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set up pre-tokenizer\n",
    "regex_pattern = r\"(\\[[^\\]]+]|Br|Cl|N|O|S|P|F|I|B|C|n|o|s|p|c|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "pre_tokenizer = pre_tokenizers.Split(Regex(regex_pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Add special tokens and post-processor\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=768)\n",
    "\n",
    "seq_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "tokenizer.post_processor = seq_processor\n",
    "\n",
    "# Test\n",
    "test_sequence = \"BrClCC[C@@H][C@H]CCC\"\n",
    "print(\"\\nTesting tokenization:\")\n",
    "print(\"Pre-tokenized:\", tokenizer.pre_tokenizer.pre_tokenize_str(test_sequence))\n",
    "encoded = tokenizer.encode(test_sequence)\n",
    "print(\"Encoded tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sample: [('[PAD]', 0), ('[UNK]', 1), ('[CLS]', 2), ('[SEP]', 3), ('[MASK]', 4)]\n",
      "Merges sample: [('=', 'O'), ('C', 'C'), ('N', 'C'), ('C', 'O'), ('c', 'c')]\n",
      "\n",
      "Testing tokenization:\n",
      "Pre-tokenized: [('Br', (0, 2)), ('Cl', (2, 4)), ('C', (4, 5)), ('C', (5, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('C', (17, 18)), ('C', (18, 19)), ('C', (19, 20))]\n",
      "Encoded tokens: ['[CLS]', 'B', '[UNK]', 'C', '[UNK]', 'C', 'C', '[UNK]', 'C', '[UNK]', '[UNK]', 'C', '[UNK]', 'C', 'C', 'C', '[SEP]']\n",
      "Token IDs: [2, 122, 1, 83, 1, 83, 83, 1, 83, 1, 1, 83, 1, 83, 83, 83, 3]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers import processors\n",
    "\n",
    "# First, let's check our input files\n",
    "import json\n",
    "with open(\"build_tokenizer/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "    print(\"Vocabulary sample:\", list(vocab.items())[:5])\n",
    "\n",
    "with open(\"build_tokenizer/merges_cleaned.txt\", \"r\") as f:\n",
    "    # make each line a tuple (first element is the first token, second element is the second token)\n",
    "    merges = [tuple(line.strip().split()) for line in f]\n",
    "    print(\"Merges sample:\", merges[:5])\n",
    "\n",
    "# Create BPE model with explicit vocab and merges\n",
    "tokenizer = Tokenizer(BPE(\n",
    "    vocab=vocab,\n",
    "    merges=merges,\n",
    "    cache_capacity=10000,\n",
    "    dropout=None,\n",
    "    unk_token=\"[UNK]\",\n",
    "    continuing_subword_prefix=\"\",\n",
    "    end_of_word_suffix=\"\",\n",
    "    fuse_unk=True\n",
    "))\n",
    "\n",
    "# Set up pre-tokenizer\n",
    "regex_pattern = r\"(\\[[^\\]]+]|Br|Cl|N|O|S|P|F|I|B|C|n|o|s|p|c|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "pre_tokenizer = pre_tokenizers.Split(Regex(regex_pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Add special tokens and post-processor\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=768)\n",
    "\n",
    "seq_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "tokenizer.post_processor = seq_processor\n",
    "\n",
    "# Test\n",
    "test_sequence = \"BrClCC[C@@H][C@H]CCC\"\n",
    "print(\"\\nTesting tokenization:\")\n",
    "print(\"Pre-tokenized:\", tokenizer.pre_tokenizer.pre_tokenize_str(test_sequence))\n",
    "encoded = tokenizer.encode(test_sequence)\n",
    "print(\"Encoded tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: CCC\n",
      "Pre-tokenized: [('CCC', (0, 3))]\n",
      "Final tokens: ['[CLS]', 'CCC', '[SEP]']\n",
      "\n",
      "Testing: CCCC\n",
      "Pre-tokenized: [('CCCC', (0, 4))]\n",
      "Final tokens: ['[CLS]', 'CCCC', '[SEP]']\n",
      "\n",
      "Testing: BrCCCB\n",
      "Pre-tokenized: [('Br', (0, 2)), ('CCC', (2, 5)), ('B', (5, 6))]\n",
      "Final tokens: ['[CLS]', 'Br', 'CCC', 'B', '[SEP]']\n",
      "\n",
      "Testing: BrClCC[C@@H][C@H]CCC\n",
      "Pre-tokenized: [('Br', (0, 2)), ('Cl', (2, 4)), ('CC', (4, 6)), ('[C@@H]', (6, 12)), ('[C@H]', (12, 17)), ('CCC', (17, 20))]\n",
      "Final tokens: ['[CLS]', 'Br', 'Cl', 'CC', '[C@@H]', '[C@H]', 'CCC', '[SEP]']\n",
      "\n",
      "Testing: C(=O)N\n",
      "Pre-tokenized: [('C(=O)N', (0, 6))]\n",
      "Final tokens: ['[CLS]', 'C(=O)N', '[SEP]']\n",
      "\n",
      "Testing: CC(=O)CC\n",
      "Pre-tokenized: [('CC(=O)', (0, 6)), ('CC', (6, 8))]\n",
      "Final tokens: ['[CLS]', 'CC(=O)', 'CC', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex, processors\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "# Create WordPiece tokenizer\n",
    "tokenizer = Tokenizer(WordPiece(\n",
    "    vocab=vocab,\n",
    "    unk_token=\"[UNK]\",\n",
    "    max_input_chars_per_word=100\n",
    "))\n",
    "\n",
    "# Chemical regex pattern with correct escaping\n",
    "chemical_pattern = r\"(\\[[^\\]]+]|C\\(=N\\)N|CCC\\(C\\)|\\(CCCN\\)|NC\\(=O\\)|C\\(C\\)=O|=C\\(N\\)N|N=C\\(N\\)|NC\\(=N\\)|C\\(=O\\)C|CS\\(=O\\)|OC\\(=O\\)|C\\(=O\\)c|c\\(=O\\)n|C\\(=O\\)O|C\\(N\\)=O|cc\\(Br\\)|CC\\(=O\\)|C\\(=O\\)N|ccc\\(C\\)|ccc\\(F\\)|c\\(=O\\)|C\\(=N\\)|c\\(O\\)c|NC\\(C\\)|n\\(C\\)c|CC\\(O\\)|cc\\(N\\)|CC\\(C\\)|cc\\(C\\)|C\\(=O\\)|cc\\(O\\)|c\\(N\\)c|c\\(Cl\\)|C\\(N\\)N|N\\(C\\)C|NC\\(N\\)|=C\\(N\\)|C\\(O\\)C|c\\(OC\\)|\\(C#N\\)|C\\(C\\)C|CC\\(N\\)|C\\(C\\)N|c\\(CO\\)|c\\(Br\\)|\\(CCO\\)|C\\(CC\\)|S\\(=O\\)|c\\(C\\)c|\\(=N\\)|c\\(O\\)|\\(Br\\)|\\(CS\\)|c\\(C\\)|\\(CC\\)|c\\(I\\)|C\\(C\\)|N\\(C\\)|C\\(O\\)|C\\(I\\)|C\\(F\\)|\\(Cl\\)|n\\(C\\)|\\(OC\\)|\\(=O\\)|c\\(F\\)|CCCN\\)|\\(=S\\)|c\\(N\\)|\\(CO\\)|C\\(N\\)|\\(C\\)|ccccc|\\(S\\)|\\(F\\)|\\(O\\)|C#N\\)|CCO\\)|\\(N\\)|C\\(=N|\\(I\\)|CSSC|=N\\)|CC=O|CCCO|Cl\\)|CCNO|=O\\)|CCSC|\\(=N|CO\\)|CCNC|CCCC|=S\\)|CN=C|CCCS|cccc|CCCN|Br\\)|cccn|CS\\)|C=CC|OC\\)|CC=C|cnn|=NC|COC|OCC|\\(O|CCS|CNc|#Cc|=CC|ccn|C=C|CSc|ccc|NCc|CCO|N=C|cnc|I\\)|CCc|OCc|CCl|ccs|COc|CCn|CSC|SCC|NCC|CCN|CNC|C#C|C=O|CNO|CCC|SSC|C#N|O=C|NOC|S\\)|csc|ncc|C\\)|N\\)|\\(C|ncn|F\\)|O\\)|N#C|nnc|CSS|cco|Cl|NC|nc|co|CS|CO|no|cc|CN|cn|SS|OC|\\)|SN|nn|CC|#C|NO|=S|NS|cs|=C|Oc|=O|oc|Nc|Cc|=N|NN|C=|C#|\\(|SC|sc|Br|N#|#N|p|O|I|N|C|s|=|c|B|S|F|n|P|#|o)\"\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Split(Regex(chemical_pattern), behavior=\"isolated\")\n",
    "\n",
    "# Add post-processor for special tokens\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# Test sequences\n",
    "test_sequences = [\n",
    "    \"CCC\",\n",
    "    \"CCCC\",\n",
    "    \"BrCCCB\",\n",
    "    \"BrClCC[C@@H][C@H]CCC\",\n",
    "    \"C(=O)N\",  # Test parentheses\n",
    "    \"CC(=O)CC\"  # Test more complex structure\n",
    "]\n",
    "\n",
    "for seq in test_sequences:\n",
    "    print(f\"\\nTesting: {seq}\")\n",
    "    pre_tokens = tokenizer.pre_tokenizer.pre_tokenize_str(seq)\n",
    "    print(\"Pre-tokenized:\", pre_tokens)\n",
    "    encoded = tokenizer.encode(seq)\n",
    "    print(\"Final tokens:\", encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "286\n",
      "N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"build_tokenizer/tokenizer.json\")\n",
    "\n",
    "# test the tokenizer\n",
    "sequence = 'N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O'\n",
    "print(len(sequence))\n",
    "\n",
    "# tokenize the sequence\n",
    "tokens = tokenizer.encode(sequence)\n",
    "print(len(tokens))\n",
    "\n",
    "# decode the tokens\n",
    "decoded = tokenizer.decode(tokens.ids)\n",
    "# replace all spaces with ''\n",
    "decoded = decoded.replace(\" \", \"\")\n",
    "# remove [CLS] and [SEP]\n",
    "decoded = decoded.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\")\n",
    "\n",
    "print(decoded)\n",
    "\n",
    "\n",
    "# check if decoded is the same as sequence\n",
    "print(decoded == sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc65c32736848bba191890a72d7be48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aaronfeller/PeptideMTR/commit/bbd6290716d787242ecbfa325211474cbb8b6161', commit_message='Add chemical WordPiece tokenizer', commit_description='', oid='bbd6290716d787242ecbfa325211474cbb8b6161', pr_url=None, repo_url=RepoUrl('https://huggingface.co/aaronfeller/PeptideMTR', endpoint='https://huggingface.co', repo_type='model', repo_id='aaronfeller/PeptideMTR'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, Regex, processors\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import pre_tokenizers\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# First, convert your tokenizer to a PreTrainedTokenizerFast\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "# Add tokenizer metadata\n",
    "fast_tokenizer.name_or_path = \"chemical-wordpiece-tokenizer\"\n",
    "fast_tokenizer.model_max_length = 768  # or whatever max length you want\n",
    "\n",
    "# Push to hub (make sure you're logged in first)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Push to hub\n",
    "fast_tokenizer.push_to_hub(\n",
    "    \"aaronfeller/PeptideMTR\",  # e.g., \"pharmapsychotic/chemical-wordpiece\"\n",
    "    commit_message=\"Add chemical WordPiece tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 107, 67, 239, 219, 107, 117, 237, 141, 239, 244, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 250, 237, 67, 149, 141, 376, 269, 237, 67, 239, 65, 107, 231, 107, 117, 237, 67, 239, 242, 117, 237, 67, 239, 237, 26, 117, 237, 67, 239, 242, 117, 237, 67, 250, 237, 67, 239, 65, 107, 231, 107, 117, 237, 243, 72, 141, 239, 219, 141, 239, 235, 238, 232, 237, 67, 239, 237, 117, 237, 141, 239, 219, 141, 239, 235, 238, 232, 237, 67, 239, 83, 149, 278, 72, 69, 117, 83, 107, 278, 69, 381, 278, 107, 117, 237, 67, 239, 277, 117, 237, 67, 239, 277, 117, 237, 67, 149, 141, 270, 269, 237, 67, 239, 242, 117, 237, 67, 250, 237, 67, 240, 237, 67, 239, 243, 238, 237, 67, 239, 254, 257, 232, 237, 67, 239, 240, 269, 237, 67, 239, 237, 117, 237, 67, 239, 254, 257, 232, 237, 67, 239, 244, 117, 237, 67, 239, 65, 107, 231, 107, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 239, 254, 257, 232, 237, 67, 239, 277, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 239, 240, 269, 237, 67, 149, 141, 270, 244, 117, 237, 67, 239, 83, 107, 278, 220, 253, 107, 117, 237, 67, 239, 243, 238, 237, 67, 240, 237, 141, 357, 237, 67, 239, 65, 107, 231, 107, 360, 237, 67, 239, 243, 232, 237, 67, 239, 277, 117, 237, 67, 239, 243, 232, 237, 67, 239, 254, 257, 232, 237, 67, 239, 65, 107, 65, 121, 65, 69, 65, 107, 324, 222, 69, 117, 244, 3]\n",
      "N 1 [C@@H] (C CC 1 ) C(=O)N [C@H] (C C(=O)O ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (CO) C(=O)N [C@@H] ( [C@H] (CC) C) C(=O)N [C@@H] (C c 1 ccccc 1 ) C(=O)N [C@@H] (C C(C)C ) C(=O)N [C@@H] (C C(=O)N 3 ) C(=O)N [C@@H] (C C(C)C ) C(=O)N [C@@H] (CO) C(=O)N [C@@H] (C c 1 ccccc 1 ) C(=O)N CC(=O) N [C@H] (C CC [C@H] (C (=O) O) N) C(=O)N [C@@H] (C C(=O)N ) C(=O)N [C@H] (C CC [C@H] (C (=O) O) N) C(=O)N [C@@H] (C C ( =C N 2 ) C 1 =C 2 C=CC =C 1 ) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] ( [C@H] (O) C) C(=O)N [C@@H] (C C(C)C ) C(=O)N [C@@H] (CO) C(=O)N [C@@H] (C) C(=O)N [C@@H] (C CC(=O) O) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C (C) C) C(=O)N [C@@H] (C C(=O)N ) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C C(=O)O ) C(=O)N [C@@H] (C c 1 ccccc 1 ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (C (C) C) C(=O)N [C@@H] ( [C@H] (O) C(=O)O ) C(=O)N [C@@H] (C C 1 =C NC =N 1 ) C(=O)N [C@@H] (C CC(=O) O) C(=O)N [C@@H] (C) C(=O)N [C@H] (CS) C(=O)N [C@@H] (C c 1 ccccc 1 Cl) C(=O)N [C@@H] (C CC(=O) N) C(=O)N [C@@H] (C CSC ) C(=O)N [C@@H] (C CC(=O) N) C(=O)N [C@@H] (C CCNC (=N) N) C(=O)N [C@@H] (C c 1 c [nH] c 2 c 1 c(N)c cc 2 ) C(=O)O\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# load the tokenizer from the hub\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"aaronfeller/PeptideMTR\")\n",
    "\n",
    "tokenizer.encode(\"CCC\")\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(\"CCC\"))\n",
    "\n",
    "sequence = 'N1[C@@H](CCC1)C(=O)N[C@H](CC(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CO)C(=O)N[C@@H]([C@H](CC)C)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(=O)N3)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](Cc1ccccc1)C(=O)NCC(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@H](CCC[C@H](C(=O)O)N)C(=O)N[C@@H](CC(=CN2)C1=C2C=CC=C1)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCSC)C(=O)N[C@@H]([C@H](O)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CO)C(=O)N[C@@H](C)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H](CC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CC(=O)O)C(=O)N[C@@H](Cc1ccccc1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](C(C)C)C(=O)N[C@@H]([C@H](O)C(=O)O)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CCC(=O)O)C(=O)N[C@@H](C)C(=O)N[C@H](CS)C(=O)N[C@@H](Cc1ccccc1Cl)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCC(=O)N)C(=O)N[C@@H](CCCNC(=N)N)C(=O)N[C@@H](Cc1c[nH]c2c1c(N)ccc2)C(=O)O'\n",
    "\n",
    "tokens = tokenizer.encode(sequence)\n",
    "print(tokens)\n",
    "\n",
    "decoded = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "\n",
    "# remove spaces in decoded\n",
    "decoded = decoded.replace(\" \", \"\")\n",
    "\n",
    "print(decoded == sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
