{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMC(\n",
       "  (embed): Embedding(64, 960)\n",
       "  (transformer): TransformerStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-29): 30 x UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=960, out_features=2880, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (q_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=960, out_features=5120, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=2560, out_features=960, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (sequence_head): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=960, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "client = ESMC.from_pretrained(\"esmc_300m\").to(\"cpu\")\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom_ESMC(\n",
      "  (embed): Embedding(64, 960)\n",
      "  (transformer): TransformerStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0-29): 30 x UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=960, out_features=2880, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
      "          (q_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryPositionalEmbeddings()\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=960, out_features=5120, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=2560, out_features=960, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sequence_head): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=960, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return F.silu(x1) * x2\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.layernorm_qkv = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim * 3, bias=False),\n",
    "        )\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.q_ln = nn.LayerNorm(embed_dim)\n",
    "        self.k_ln = nn.LayerNorm(embed_dim)\n",
    "        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim, max_seq_len=max_seq_len)\n",
    "\n",
    "    def forward(self, x, input_pos=None):\n",
    "        B, T, C = x.shape  # Batch size, sequence length, embed dim\n",
    "        qkv = self.layernorm_qkv(x).view(B, T, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "\n",
    "        # Apply rotary embeddings to queries and keys\n",
    "        q = self.rotary(q, input_pos=input_pos)\n",
    "        k = self.rotary(k, input_pos=input_pos)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = torch.einsum(\"bnqd,bnkd->bnqk\", q, k) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.einsum(\"bnqk,bnvd->bnqd\", attn_weights, v)\n",
    "        attn_output = attn_output.contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "class UnifiedTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_hidden_dim, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, max_seq_len)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim, bias=False),\n",
    "            SwiGLU(),\n",
    "            nn.Linear(ffn_hidden_dim // 2, embed_dim, bias=False),  # SwiGLU halves hidden dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x, input_pos=None):\n",
    "        x = x + self.attn(x, input_pos=input_pos)\n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class TransformerStack(nn.Module):\n",
    "    def __init__(self, num_blocks, embed_dim, num_heads, ffn_hidden_dim, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [UnifiedTransformerBlock(embed_dim, num_heads, ffn_hidden_dim, max_seq_len) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, input_pos=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, input_pos=input_pos)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Custom_ESMC(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_blocks, num_heads, ffn_hidden_dim, output_dim, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer = TransformerStack(num_blocks, embed_dim, num_heads, ffn_hidden_dim, max_seq_len)\n",
    "        self.sequence_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, output_dim, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, input_pos=None):\n",
    "        x = self.embed(x)\n",
    "        x = self.transformer(x, input_pos=input_pos)\n",
    "        return self.sequence_head(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Custom_ESMC(\n",
    "    vocab_size=64,\n",
    "    embed_dim=960,\n",
    "    num_blocks=30,\n",
    "    num_heads=8,  # Example: Adjust num_heads as needed\n",
    "    ffn_hidden_dim=5120,\n",
    "    output_dim=64,\n",
    "    max_seq_len=4096,  # Ensure this matches the maximum sequence length expected\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096, 64])\n"
     ]
    }
   ],
   "source": [
    "# make a fake tensor input to the model\n",
    "x = torch.randint(0, 64, (1, 4096))\n",
    "out = model(x)\n",
    "print(out.shape)  # torch.Size([1, 4096, 64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4917, -0.0814, -0.7978,  ..., -0.4080, -0.4066, -1.9724],\n",
       "         [ 0.0868,  0.2095,  1.0585,  ...,  0.0946, -0.0400,  0.5337],\n",
       "         [ 0.0096,  0.3399, -0.2859,  ...,  0.2592, -0.2552,  0.2571],\n",
       "         ...,\n",
       "         [-0.1556, -0.0589, -0.8261,  ...,  0.3990, -0.7987,  0.3712],\n",
       "         [-0.6930,  0.4637, -0.2504,  ...,  0.0775,  0.0601, -0.9776],\n",
       "         [-0.3557,  0.2114,  0.5776,  ...,  0.8534,  0.0169,  0.0795]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333055744"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count model parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
